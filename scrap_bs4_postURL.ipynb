{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para verificar si una página existe\n",
    "def pagina_existe(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        return False\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    no_results = soup.find('div', class_='no-results')\n",
    "    if no_results:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para obtener detalles de un inmueble\n",
    "def obtener_detalles_inmueble(url_inmueble):\n",
    "    try:\n",
    "        response = requests.get(url_inmueble)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        try:\n",
    "            nombre = soup.find(\"h1\", class_=\"ad-title\")\n",
    "            nombre = nombre.text.strip() if nombre else \"Error al acceder al inmueble\"\n",
    "        except:\n",
    "            nombre = \"Error al acceder al inmueble\"\n",
    "\n",
    "        try:\n",
    "            agencia = soup.find(\"p\", class_=\"owner-info__name\")\n",
    "            agencia = agencia.find(\"a\").text.strip() if agencia and agencia.find(\"a\") else \"Agencia no disponible\"\n",
    "        except:\n",
    "            agencia = \"Agencia no disponible\"\n",
    "\n",
    "        try:\n",
    "            features_summary = soup.find(\"ul\", class_=\"features-summary\")\n",
    "            precio_m2 = None\n",
    "            if features_summary:\n",
    "                items = features_summary.find_all(\"li\", class_=\"features-summary__item\")\n",
    "                for item in items:\n",
    "                    if \"€/m²\" in item.text:\n",
    "                        precio_m2 = item.text.strip()\n",
    "                        break\n",
    "            precio_m2 = precio_m2 if precio_m2 else \"Error al procesar precio m²\"\n",
    "        except:\n",
    "            precio_m2 = \"Error al procesar precio m²\"\n",
    "        \n",
    "        try:\n",
    "            precio = soup.find(\"div\", class_=\"price__value jsPriceValue\")\n",
    "            precio = precio.text.strip() if precio else \"Error al procesar precio\"\n",
    "        except:\n",
    "            precio = \"Error al procesar precio\"\n",
    "        \n",
    "        try:\n",
    "            superficie = soup.find(\"span\", class_=\"features__value\")\n",
    "            superficie = superficie.text.strip() if superficie else \"Error al procesar superficie\"\n",
    "        except:\n",
    "            superficie = \"Error al procesar superficie\"\n",
    "        \n",
    "        try:\n",
    "            actualizacion = soup.find(\"div\", class_=\"details__block last-update\")\n",
    "            actualizacion = actualizacion.text.strip().replace(\"Última actualización\\n\", \"\").strip() if actualizacion else \"Error al procesar actualizacion\"\n",
    "        except:\n",
    "            actualizacion = \"Error al procesar actualizacion\"\n",
    "\n",
    "        try:\n",
    "            consumo = soup.find_all(\"span\", class_=\"energy-certificate__tag\")\n",
    "            consumo_etiquetas = []\n",
    "            for etiqueta in consumo:\n",
    "                if 'energy-certificate__tag--' in etiqueta['class'][1]:\n",
    "                    letra_consumo = etiqueta['class'][1].split('--')[1].strip()\n",
    "                    if letra_consumo in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]: \n",
    "                        consumo_etiquetas.append(letra_consumo.upper())\n",
    "            consumo = ', '.join(consumo_etiquetas) if consumo_etiquetas else \"Error al procesar consumo\"\n",
    "        except:\n",
    "            consumo = \"Error al procesar consumo\"\n",
    "\n",
    "        try:\n",
    "            emisiones = soup.find_all(\"span\", class_=\"energy-certificate__tag\")\n",
    "            emisiones_etiquetas = []\n",
    "            for etiqueta in emisiones:\n",
    "                if 'energy-certificate__tag--' in etiqueta['class'][1]:\n",
    "                    letra_emisiones = etiqueta['class'][1].split('--')[1].strip()\n",
    "                    if letra_emisiones in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]:\n",
    "                        emisiones_etiquetas.append(letra_emisiones.upper())\n",
    "            emisiones = ', '.join(emisiones_etiquetas) if emisiones_etiquetas else \"Error al procesar emisiones\"\n",
    "        except:\n",
    "            emisiones = \"Error al procesar emisiones\"\n",
    "\n",
    "        try:\n",
    "            caracteristicas = soup.find(\"div\", class_=\"features-container\")\n",
    "            detalles = {}\n",
    "            if caracteristicas:\n",
    "                secciones = caracteristicas.find_all(\"div\", class_=\"features__content\")\n",
    "                for seccion in secciones:\n",
    "                    features = seccion.find_all(\"div\", class_=\"features__feature\")\n",
    "                    for feature in features:\n",
    "                        label = feature.find(\"span\", class_=\"features__label\").text.strip().replace(\":\", \"\")\n",
    "                        value = feature.find(\"span\", class_=\"features__value\").text.strip() if feature.find(\"span\", \"features__value\") else \"Si\"\n",
    "                        detalles[label] = value\n",
    "        except:\n",
    "            detalles = {}\n",
    "\n",
    "        # Aquí vamos a extraer el Código Postal e Identificador mediante la URL de cada inmueble\n",
    "        try:\n",
    "            codigo_postal_match = re.search(r'(\\d{5})-', url_inmueble)\n",
    "            codigo_postal = codigo_postal_match.group(1) if codigo_postal_match else 'NaN'\n",
    "        except Exception as e:\n",
    "            codigo_postal = f\"Error al extraer código postal: {e}\"\n",
    "        \n",
    "        try:\n",
    "            identificador_match = re.search(r'-(\\d+_\\d+)', url_inmueble)\n",
    "            identificador = identificador_match.group(1) if identificador_match else 'NaN'\n",
    "        except Exception as e:\n",
    "            identificador = f\"Error al extraer identificador: {e}\"\n",
    "        \n",
    "        # Aquí extraemos el timestamp de cada extracción de datos, que nos será útil a la hora de actualizar la db\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        # Aquí extraemos las coordenadas (latitud y longitud)\n",
    "        try:\n",
    "            location_div = soup.find(\"div\", class_=\"location\")\n",
    "            data_params = location_div[\"data-params\"]\n",
    "            \n",
    "            lat_match = re.search(r'latitude=([-0-9.]+)', data_params)\n",
    "            lon_match = re.search(r'longitude=([-0-9.]+)', data_params)\n",
    "            \n",
    "            latitud = lat_match.group(1) if lat_match else 'NaN'\n",
    "            longitud = lon_match.group(1) if lon_match else 'NaN'\n",
    "            \n",
    "            coordenadas = f\"{latitud}, {longitud}\"\n",
    "        except Exception as e:\n",
    "            coordenadas = f\"Error al extraer coordenadas: {e}\"\n",
    "            \n",
    "        return nombre, agencia, precio_m2, precio, superficie, actualizacion, consumo, emisiones, detalles, codigo_postal, identificador, timestamp, coordenadas\n",
    "    except Exception as e:\n",
    "        return (\"Error al acceder al inmueble\",) * 9 + ({}, \"\", \"\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'), \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Función para obtener inmuebles de URLs\n",
    "def obtener_inmuebles_de_urls(urls_a_scrapear):\n",
    "    todos_los_inmuebles = []\n",
    "    columnas = set()\n",
    "\n",
    "    for url_pagina in urls_a_scrapear:\n",
    "        print(f\"Procesando la URL: {url_pagina}\")\n",
    "        \n",
    "        pagina = 1\n",
    "        while True:\n",
    "            try:\n",
    "                response = requests.get(url_pagina)\n",
    "                if response.status_code == 404:\n",
    "                    print(f\"Página no encontrada: {url_pagina}\")\n",
    "                    break\n",
    "                soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "                \n",
    "                titulos_soup = soup.find_all(\"a\", class_=\"ad-preview__title\")\n",
    "                \n",
    "                for titulo_soup in titulos_soup:\n",
    "                    nombre = titulo_soup.text.strip()\n",
    "                    href = \"https://www.pisos.com\" + titulo_soup['href']\n",
    "                    nombre_detalle, agencia, precio_m2, precio, superficie, actualizacion, consumo, emisiones, detalles, codigo_postal, identificador, timestamp, coordenadas = obtener_detalles_inmueble(href)\n",
    "                    \n",
    "                    # corregido extraer ubic\n",
    "                    ubicacion = url_pagina.split('/')[-3].split('-')[-1]\n",
    "                    \n",
    "                    inmueble = {\n",
    "                        \"nombre\": nombre,\n",
    "                        \"agencia\": agencia,\n",
    "                        \"precio_m2\": precio_m2,\n",
    "                        \"precio\": precio,\n",
    "                        \"superficie\": superficie,\n",
    "                        \"href\": href,\n",
    "                        \"actualizacion\": actualizacion,\n",
    "                        \"consumo\": consumo,\n",
    "                        \"emisiones\": emisiones,\n",
    "                        \"ubicacion\": ubicacion,\n",
    "                        \"codigo_postal\": codigo_postal,\n",
    "                        \"identificador\": identificador,\n",
    "                        \"timestamp\": timestamp,\n",
    "                        \"coordenadas\": coordenadas\n",
    "                    }\n",
    "                    \n",
    "                    inmueble.update(detalles)\n",
    "                    todos_los_inmuebles.append(inmueble)\n",
    "                    columnas.update(inmueble.keys())\n",
    "                \n",
    "                pagina += 1\n",
    "                if pagina > 2:  # Limitar a 2 páginas para la prueba\n",
    "                    break\n",
    "                url_pagina = f\"{url_pagina.rsplit('/', 2)[0]}/{pagina}/\"\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error al procesar la URL {url_pagina}: {e}\")\n",
    "                break\n",
    "            time.sleep(1) \n",
    "    \n",
    "    df_inmuebles = pd.DataFrame(todos_los_inmuebles, columns=list(columnas))\n",
    "    return df_inmuebles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leer el archivo txt con las URLs y asignarlas a la lista 'urls'\n",
    "file_path = 'urls_a_scrapear.txt'\n",
    "try:\n",
    "    with open(file_path, 'r') as file:\n",
    "        urls = file.read().splitlines()\n",
    "    print(f\"Se han cargado {len(urls)} URLs desde el archivo.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"El archivo {file_path} no se encontró.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar lista de URLs a scrapear\n",
    "urls_a_scrapear = []\n",
    "\n",
    "for base_url in urls:     # Intentar si hay hasta la página 100\n",
    "    for i in range(1, 101):\n",
    "        url = f\"{base_url}{i}/\"\n",
    "        if pagina_existe(url):\n",
    "            urls_a_scrapear.append(url)\n",
    "        else:\n",
    "            print(f\"Página no encontrada: {url}\")\n",
    "            break  # Si no existe la página, lo dejo \n",
    "print(f\"Hay {len(urls_a_scrapear)} URLs a scrapear\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener inmuebles de las URLs generadas\n",
    "df_inmuebles = obtener_inmuebles_de_urls(urls_a_scrapear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inmuebles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inmuebles.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seleccionado = df_inmuebles[[\"identificador\", \"nombre\", \"ubicacion\", \"actualizacion\", \"precio\", \"timestamp\", \"coordenadas\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_seleccionado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inmuebles[\"ubicacion\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inmuebles.to_csv('compras_completo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faa10f75de45951c7c3939f3103988890fef5be60eea20e39c3d3747e4fc449a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
