{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aa6a3f-6254-42ab-a81c-323c8c6c4507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17157df3-8af6-4ad8-a45b-63215cc204ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#He implementado try:, except: en la obtención de cada categoría, consejo de Dmitry\n",
    "def obtener_detalles_inmueble(url_inmueble):\n",
    "    try:\n",
    "        response = requests.get(url_inmueble)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        try:\n",
    "            nombre = soup.find(\"h1\", class_=\"ad-title\")\n",
    "            nombre = nombre.text.strip() if nombre else \"Error al acceder al inmueble\"\n",
    "        except:\n",
    "            nombre = \"Error al acceder al inmueble\"\n",
    "\n",
    "        try:\n",
    "            agencia = soup.find(\"p\", class_=\"owner-info__name\")\n",
    "            agencia = agencia.find(\"a\").text.strip() if agencia and agencia.find(\"a\") else \"Agencia no disponible\"\n",
    "        except:\n",
    "            agencia = \"Agencia no disponible\"\n",
    "\n",
    "        try:\n",
    "            features_summary = soup.find(\"ul\", class_=\"features-summary\")\n",
    "            precio_m2 = None\n",
    "            if features_summary:\n",
    "                items = features_summary.find_all(\"li\", class_=\"features-summary__item\")\n",
    "                for item in items:\n",
    "                    if \"€/m²\" in item.text:\n",
    "                        precio_m2 = item.text.strip()\n",
    "                        break\n",
    "            precio_m2 = precio_m2 if precio_m2 else \"Error al procesar precio m²\"\n",
    "        except:\n",
    "            precio_m2 = \"Error al procesar precio m²\"\n",
    "        \n",
    "        try:\n",
    "            precio = soup.find(\"div\", class_=\"price__value jsPriceValue\")\n",
    "            precio = precio.text.strip() if precio else \"Error al procesar precio\"\n",
    "        except:\n",
    "            precio = \"Error al procesar precio\"\n",
    "        \n",
    "        try:\n",
    "            superficie = soup.find(\"span\", class_=\"features__value\")\n",
    "            superficie = superficie.text.strip() if superficie else \"Error al procesar superficie\"\n",
    "        except:\n",
    "            superficie = \"Error al procesar superficie\"\n",
    "        \n",
    "        try:\n",
    "            actualizacion = soup.find(\"div\", class_=\"details__block last-update\")\n",
    "            actualizacion = actualizacion.text.strip().replace(\"Última actualización\\n\", \"\").strip() if actualizacion else \"Error al procesar actualizacion\"\n",
    "        except:\n",
    "            actualizacion = \"Error al procesar actualizacion\"\n",
    "\n",
    "        try:\n",
    "            consumo = soup.find_all(\"span\", class_=\"energy-certificate__tag\")\n",
    "            consumo_etiquetas = []\n",
    "            for etiqueta in consumo:\n",
    "                if 'energy-certificate__tag--' in etiqueta['class'][1]:\n",
    "                    letra_consumo = etiqueta['class'][1].split('--')[1].strip()\n",
    "                    if letra_consumo in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]: \n",
    "                        consumo_etiquetas.append(letra_consumo.upper())\n",
    "            consumo = ', '.join(consumo_etiquetas) if consumo_etiquetas else \"Error al procesar consumo\"\n",
    "        except:\n",
    "            consumo = \"Error al procesar consumo\"\n",
    "\n",
    "        try:\n",
    "            emisiones = soup.find_all(\"span\", class_=\"energy-certificate__tag\")\n",
    "            emisiones_etiquetas = []\n",
    "            for etiqueta in emisiones:\n",
    "                if 'energy-certificate__tag--' in etiqueta['class'][1]:\n",
    "                    letra_emisiones = etiqueta['class'][1].split('--')[1].strip()\n",
    "                    if letra_emisiones in [\"a\", \"b\", \"c\", \"d\", \"e\", \"f\", \"g\"]:\n",
    "                        emisiones_etiquetas.append(letra_emisiones.upper())\n",
    "            emisiones = ', '.join(emisiones_etiquetas) if emisiones_etiquetas else \"Error al procesar emisiones\"\n",
    "        except:\n",
    "            emisiones = \"Error al procesar emisiones\"\n",
    "\n",
    "        try:\n",
    "            caracteristicas = soup.find(\"div\", class_=\"features-container\")\n",
    "            detalles = {}\n",
    "            if caracteristicas:\n",
    "                secciones = caracteristicas.find_all(\"div\", class_=\"features__content\")\n",
    "                for seccion in secciones:\n",
    "                    features = seccion.find_all(\"div\", class_=\"features__feature\")\n",
    "                    for feature in features:\n",
    "                        label = feature.find(\"span\", class_=\"features__label\").text.strip().replace(\":\", \"\")\n",
    "                        value = feature.find(\"span\", class_=\"features__value\").text.strip() if feature.find(\"span\", class_=\"features__value\") else \"Si\"\n",
    "                        detalles[label] = value\n",
    "        except:\n",
    "            detalles = {}\n",
    "\n",
    "        # Aquí vamos a extraer el Código Postal e Identificador mediante la URL de cada inmueble\n",
    "        try:\n",
    "            codigo_postal_match = re.search(r'(\\d{5})-', url_inmueble)\n",
    "            codigo_postal = codigo_postal_match.group(1) if codigo_postal_match else 'NaN'\n",
    "        except Exception as e:\n",
    "            codigo_postal = f\"Error al extraer código postal: {e}\"\n",
    "        \n",
    "        try:\n",
    "            identificador_match = re.search(r'-(\\d+_\\d+)', url_inmueble)\n",
    "            identificador = identificador_match.group(1) if identificador_match else 'NaN'\n",
    "        except Exception as e:\n",
    "            identificador = f\"Error al extraer identificador: {e}\"\n",
    "        \n",
    "        # Aquí extraemos el timestamp de cada extracción de datos, que nos será util a la hora de actualizar la db\n",
    "        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "            \n",
    "        return nombre, agencia, precio_m2, precio, superficie, actualizacion, consumo, emisiones, detalles, codigo_postal, identificador, timestamp\n",
    "    except Exception as e:\n",
    "        return (\"Error al acceder al inmueble\",) * 9 + ({}, \"\", \"\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f92d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_inmuebles_paginas(base_url, paginas=2):\n",
    "    todos_los_inmuebles = []\n",
    "    columnas = set()\n",
    "\n",
    "    for i in range(1, paginas + 1):\n",
    "        print(f\"Procesando nueva página\")\n",
    "        url_pagina = f\"{base_url}{i}/\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url_pagina)\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            titulos_soup = soup.find_all(\"a\", class_=\"ad-preview__title\")\n",
    "            \n",
    "            for titulo_soup in titulos_soup:\n",
    "                nombre = titulo_soup.text.strip()\n",
    "                href = \"https://www.pisos.com\" + titulo_soup['href']\n",
    "                nombre_detalle, agencia, precio_m2, precio, superficie, actualizacion, consumo, emisiones, detalles, codigo_postal, identificador, timestamp = obtener_detalles_inmueble(href)\n",
    "                \n",
    "                ubicacion = base_url.split('/')[-2].split('-')[-1]\n",
    "                \n",
    "                inmueble = {\n",
    "                    \"nombre\": nombre,\n",
    "                    \"agencia\": agencia,\n",
    "                    \"precio_m2\": precio_m2,\n",
    "                    \"precio\": precio,\n",
    "                    \"superficie\": superficie,\n",
    "                    \"href\": href,\n",
    "                    \"actualizacion\": actualizacion,\n",
    "                    \"consumo\": consumo,\n",
    "                    \"emisiones\": emisiones,\n",
    "                    \"ubicacion\": ubicacion,\n",
    "                    \"codigo_postal\": codigo_postal,\n",
    "                    \"identificador\": identificador,\n",
    "                    \"timestamp\": timestamp\n",
    "                }\n",
    "                \n",
    "                inmueble.update(detalles)\n",
    "                todos_los_inmuebles.append(inmueble)\n",
    "                columnas.update(inmueble.keys())\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar la página {i}: {e}\")\n",
    "        \n",
    "        time.sleep(2)\n",
    "    \n",
    "    df_compras = pd.DataFrame(todos_los_inmuebles, columns=list(columnas))\n",
    "    return df_compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb28aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí vamos a hacer un diccionario con las URLs de las zonas de los inmuebles que queremos obtener, \n",
    "# con la cantidad de paginas correspondientes\n",
    "urls_zonas_paginas = {\n",
    "    \"https://www.pisos.com/venta/pisos-zaragoza/\": 1,\n",
    "    \"https://www.pisos.com/venta/pisos-huesca/\": 1,\n",
    "    \"https://www.pisos.com/venta/pisos-teruel/\": 1\n",
    "}\n",
    "\n",
    "# Aquí creo una función que recorra cada una de las paginas de las URLs del diccionario \n",
    "def obtener_inmuebles_varias_zonas(urls_zonas_paginas):\n",
    "    todos_los_inmuebles = []\n",
    "    columnas = set()\n",
    "    \n",
    "    for base_url, paginas in urls_zonas_paginas.items():\n",
    "        inmuebles_zona = obtener_inmuebles_paginas(base_url, paginas)\n",
    "        todos_los_inmuebles.extend(inmuebles_zona.to_dict('records'))\n",
    "        if not inmuebles_zona.empty:\n",
    "            columnas.update(inmuebles_zona.columns)\n",
    "    \n",
    "    df_compras = pd.DataFrame(todos_los_inmuebles, columns=list(columnas))\n",
    "    return df_compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b7d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_compras = obtener_inmuebles_varias_zonas(urls_zonas_paginas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d248e7c4-1919-4a68-9387-a07749ec2161",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6e79e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compras.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c9cf30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compras.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37307f6f-909a-4966-8542-1c3a63ac99c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Guardamos el df resultante en un .csv\n",
    "df_compras.to_csv('compras_scrap_completo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "faa10f75de45951c7c3939f3103988890fef5be60eea20e39c3d3747e4fc449a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
